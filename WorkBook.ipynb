{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Toxic Comments Data : \n",
      "\t Toxic : 9237\t Non-Toxic : 86614 \n",
      "\n",
      "Total Number of Severe_Toxic Comments Data : \n",
      "\t Severe_Toxic : 965\t Non-Severe_Toxic : 94886 \n",
      "\n",
      "Total Number of Obscene Comments Data : \n",
      "\t Obscene : 5109\t Non-Obscene : 90742 \n",
      "\n",
      "Total Number of Threat Comments Data : \n",
      "\t Threat : 305\t Non-Threat : 95546 \n",
      "\n",
      "Total Number of Insult Comments Data : \n",
      "\t Insult : 4765\t Non-Insult : 91086 \n",
      "\n",
      "Total Number of Identity_Hate Comments Data : \n",
      "\t Identity_Hate : 814\t Non-Identity_Hate : 95037 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## Author : Daniel Dsouza\n",
    "## Title : Kaggle Toxic Comment Classification Challenge\n",
    "########################################\n",
    "\n",
    "## Libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "\n",
    "## File Paths\n",
    "train_data_path = './train.csv'\n",
    "test_data_path = './test.csv'\n",
    "\n",
    "# Use Pandas to read it as DataFrames\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "\n",
    "# Split the comments up into their individual categories \n",
    "\n",
    "#Toxic Comments\n",
    "toxic_yes = train_data.loc[train_data['toxic']==1,:]\n",
    "toxic_no = train_data.loc[train_data['toxic']==0,:]\n",
    "#Severe_Toxic Comments\n",
    "sev_toxic_yes = train_data.loc[train_data['severe_toxic']==1,:]\n",
    "sev_toxic_no = train_data.loc[train_data['severe_toxic']==0,:]\n",
    "#Obscene Comments\n",
    "obscene_yes = train_data.loc[train_data['obscene']==1,:]\n",
    "obscene_no = train_data.loc[train_data['obscene']==0,:]\n",
    "#Threat Comments\n",
    "threat_yes = train_data.loc[train_data['threat']==1,:]\n",
    "threat_no = train_data.loc[train_data['threat']==0,:]\n",
    "#Insult Comments\n",
    "insult_yes = train_data.loc[train_data['insult']==1,:]\n",
    "insult_no = train_data.loc[train_data['insult']==0,:]\n",
    "#Identity Hate Comments\n",
    "id_hate_yes = train_data.loc[train_data['identity_hate']==1,:]\n",
    "id_hate_no = train_data.loc[train_data['identity_hate']==0,:]\n",
    "\n",
    "\n",
    "# List the number of comments in each category \n",
    "\n",
    "#Toxic Comments\n",
    "print(\"Total Number of Toxic Comments Data : \\n\\t Toxic : {0}\\t Non-Toxic : {1} \\n\".format(len(toxic_yes),len(toxic_no)))\n",
    "#Severe_Toxic Comments\n",
    "print(\"Total Number of Severe_Toxic Comments Data : \\n\\t Severe_Toxic : {0}\\t Non-Severe_Toxic : {1} \\n\".format(len(sev_toxic_yes),len(sev_toxic_no)))\n",
    "#Obscene Comments\n",
    "print(\"Total Number of Obscene Comments Data : \\n\\t Obscene : {0}\\t Non-Obscene : {1} \\n\".format(len(obscene_yes),len(obscene_no)))\n",
    "#Threat Comments\n",
    "print(\"Total Number of Threat Comments Data : \\n\\t Threat : {0}\\t Non-Threat : {1} \\n\".format(len(threat_yes),len(threat_no)))\n",
    "#Insult Comments\n",
    "print(\"Total Number of Insult Comments Data : \\n\\t Insult : {0}\\t Non-Insult : {1} \\n\".format(len(insult_yes),len(insult_no)))\n",
    "#Identity_Hate Comments\n",
    "print(\"Total Number of Identity_Hate Comments Data : \\n\\t Identity_Hate : {0}\\t Non-Identity_Hate : {1} \\n\".format(len(id_hate_yes),len(id_hate_no)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regex Code to clean up internet comments\n",
    "regex_cleanup = {r'(.)\\1{3,}':r'\\1',\n",
    "        r'!!+':'! ',\n",
    "        r'https?([^ ]+)':'',\n",
    "        r'[a-zA-Z]{15,}':'',\n",
    "        r'\\n':' ',\n",
    "        r'\"\"+':'\"',\n",
    "        r'\\\\+':' ',\n",
    "#         r'\\.\\.+':'.',\n",
    "        r'(\\.){2}':' ',\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}':'',\n",
    "        r'\\.\\d{,2}':'',\n",
    "        r'([a-zA-Z]{1,})+([0-9]{1,})':r'\\1',\n",
    "        r'(..)\\1{2}':'$$$',\n",
    "#         r'[\\s]{2,}':' ',\n",
    "#         r'\\d{1,}':''\n",
    "          }\n",
    "\n",
    "# Function to remove punctuations in text\n",
    "def remove_puncts(tex):\n",
    "    for i in tex:\n",
    "        for j in string.punctuation:\n",
    "            if j in tex:\n",
    "                tex = tex.replace(j,'')\n",
    "    return tex\n",
    "\n",
    "# Main cleaning function.\n",
    "def clean_up(text):\n",
    "    for i,j in regex_cleanup.items():\n",
    "#         print(i,j)\n",
    "#         print(text)\n",
    "        temp = re.sub(i,j,text)\n",
    "        text = temp\n",
    "    clean_text = remove_puncts(text)\n",
    "    return clean_text.lower()\n",
    "\n",
    "# List of texts extension for clean_up\n",
    "def clean_up_all(texts):\n",
    "#     return [clean_up(text) for text in texts]\n",
    "    cleaned_texts = []\n",
    "    for i in trange(len(texts)):\n",
    "#         print(i)\n",
    "        uniq_words = set(nltk.word_tokenize(texts[i]))\n",
    "        if len(uniq_words) < 10 :\n",
    "            current_text = ' '.join(list(uniq_words))\n",
    "        else:\n",
    "            current_text = texts[i]\n",
    "        cleaned_texts.append(clean_up(current_text))\n",
    "    return cleaned_texts\n",
    "\n",
    "# Save list as a pickle file( serial save)\n",
    "def save_as_pickle(filename,listname):\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(listname,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 66/814 [00:00<00:02, 323.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up Comments \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [00:02<00:00, 311.65it/s]\n",
      "  0%|          | 61/95037 [00:00<06:06, 258.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Several Toxic YES Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 17164/95037 [01:11<05:11, 249.80it/s]"
     ]
    }
   ],
   "source": [
    "print(\"\\nCleaning up Comments \\n\")\n",
    "id_hate_com = clean_up_all(list(id_hate_yes['comment_text']))\n",
    "print(\"\\n Several Toxic YES Done!\")\n",
    "non_id_hate_com = clean_up_all(list(id_hate_no['comment_text']))\n",
    "print(\"\\n Several Toxic NO Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_as_pickle('non_id_hate_comm.pkl',non_id_hate_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_id_hate_com[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,j in regex_cleanup.items():\n",
    "    print(i,\" to\",j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = HTMLParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_hate_comm[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
